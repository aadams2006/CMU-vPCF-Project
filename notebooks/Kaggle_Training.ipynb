{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c98dcda",
   "metadata": {},
   "source": [
    "\n",
    "# Kaggle Cloud Training Workflow\n",
    "\n",
    "This notebook orchestrates remote training on Kaggle's GPU or TPU runtimes. It\n",
    "assumes that the repository contents (including the `src/` modules) are available\n",
    "inside the Kaggle workspace â€“ either by uploading the project as a dataset or by\n",
    "linking a GitHub repository through Kaggle Notebooks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0acef7c8",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Environment setup\n",
    "\n",
    "Run the cell below to install project dependencies and register the `src/`\n",
    "modules on the Python path. When executing inside Kaggle, this will install the\n",
    "requirements into the session sandbox.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1714d542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "if not (PROJECT_ROOT / 'src').exists():\n",
    "    PROJECT_ROOT = PROJECT_ROOT.parent\n",
    "\n",
    "REQUIREMENTS = PROJECT_ROOT / 'requirements.txt'\n",
    "if REQUIREMENTS.exists():\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', '-r', str(REQUIREMENTS)])\n",
    "\n",
    "if str(PROJECT_ROOT / 'src') not in sys.path:\n",
    "    sys.path.append(str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "print(f\"Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c9822c",
   "metadata": {},
   "source": [
    "\n",
    "## 2. Configure data and output directories\n",
    "\n",
    "Update the variables in the next cell to point at your dataset (typically\n",
    "mounted beneath `/kaggle/input`) and to define where training artifacts should\n",
    "be written (commonly `/kaggle/working/results`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efff0d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to the feature matrix. Replace the placeholder with your dataset name.\n",
    "# Example: Path('/kaggle/input/vpcf-dataset/x.npy')\n",
    "data_path = Path('/kaggle/input/YOUR_DATASET_NAME/x.npy')\n",
    "\n",
    "# Optional: provide the path to ground-truth labels if available.\n",
    "# Example: Path('/kaggle/input/vpcf-dataset/y.npy')\n",
    "labels_path = None  # set to Path('...') when labels are present.\n",
    "\n",
    "# Where to store model checkpoints and logs.\n",
    "results_root = Path('/kaggle/working/results')\n",
    "results_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print('Feature matrix:', data_path)\n",
    "print('Labels path   :', labels_path)\n",
    "print('Results folder:', results_root)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603adb7",
   "metadata": {},
   "source": [
    "\n",
    "## 3. Load the dataset\n",
    "\n",
    "The cell below loads the feature matrix and (optionally) label array. Adjust the\n",
    "`astype` target if you require a different dtype.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42caf363",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "if not data_path.exists():\n",
    "    raise FileNotFoundError(f'Update data_path to match your Kaggle dataset. {data_path} not found.')\n",
    "\n",
    "x = np.load(data_path).astype('float32')\n",
    "print('Loaded features with shape:', x.shape)\n",
    "\n",
    "if labels_path is not None:\n",
    "    if not labels_path.exists():\n",
    "        raise FileNotFoundError(f'Labels path {labels_path} does not exist.')\n",
    "    y = np.load(labels_path)\n",
    "    print('Loaded labels with shape:', y.shape)\n",
    "else:\n",
    "    y = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c52f496",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Train DEC\n",
    "\n",
    "This block performs autoencoder pretraining followed by DEC clustering. Adjust\n",
    "the hyperparameters (e.g., `dims`, `n_clusters`, `maxiter`) to suit your data.\n",
    "Results and logs are saved beneath the configured `results_root`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8944db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from DEC import DEC\n",
    "\n",
    "dims = [x.shape[1], 500, 500, 2000, 10]\n",
    "dec_save_dir = results_root / 'dec'\n",
    "dec = DEC(dims=dims, n_clusters=10, save_dir=str(dec_save_dir))\n",
    "\n",
    "dec.pretrain(x, epochs=50, batch_size=256)\n",
    "dec.compile(optimizer='sgd')\n",
    "dec_labels = dec.fit(x, y=y, maxiter=8000, update_interval=200, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf5ce66",
   "metadata": {},
   "source": [
    "\n",
    "## 5. Train IDEC\n",
    "\n",
    "The IDEC workflow reuses the autoencoder weights (if available) and optimizes an\n",
    "augmented objective. Reduce the epochs or `maxiter` values if you are exploring\n",
    "interactively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ebb4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IDEC import IDEC\n",
    "\n",
    "dims = [x.shape[1], 500, 500, 2000, 10]\n",
    "idec_save_dir = results_root / 'idec'\n",
    "idec = IDEC(dims=dims, n_clusters=10, save_dir=str(idec_save_dir))\n",
    "\n",
    "if not idec.pretrained:\n",
    "    idec.pretrain(x, epochs=50, batch_size=256)\n",
    "\n",
    "idec.compile(optimizer='sgd')\n",
    "idec_labels = idec.fit(x, y=y, maxiter=8000, update_interval=200, batch_size=256)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce3d293",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Run DBSCAN baseline\n",
    "\n",
    "DBSCAN offers a non-parametric baseline that can highlight structure without\n",
    "requiring neural network training. Tune `eps` and `min_samples` for your data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20d49a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from DBSCANModel import DBSCANClustering\n",
    "\n",
    "# Hyperparameters are sensitive to feature scaling; adjust as necessary.\n",
    "dbscan = DBSCANClustering(eps=0.5, min_samples=5, scale=True)\n",
    "\n",
    "dbscan_labels = dbscan.fit(x)\n",
    "print('Unique cluster labels (including noise = -1):', sorted(set(dbscan_labels.tolist())))\n",
    "\n",
    "if y is not None:\n",
    "    metrics = dbscan.evaluate(y)\n",
    "    print('DBSCAN metrics (noise excluded):', metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399bec80",
   "metadata": {},
   "source": [
    "\n",
    "## 7. Inspect generated artifacts\n",
    "\n",
    "Use the helper below to list the files that were produced during training. The\n",
    "paths correspond to Kaggle's working directory, so you can add selected outputs\n",
    "(e.g., final weights or CSV logs) to the notebook results section for download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1234d2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for folder in ['dec', 'idec']:\n",
    "    target = results_root / folder\n",
    "    if not target.exists():\n",
    "        continue\n",
    "    print(f'\n",
    "Artifacts for {folder.upper()}:')\n",
    "    for path in sorted(target.glob('**/*')):\n",
    "        if path.is_file():\n",
    "            size_kb = path.stat().st_size / 1024\n",
    "            print(f'  {path.relative_to(results_root)} ({size_kb:.1f} KB)')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
